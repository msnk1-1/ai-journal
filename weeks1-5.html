<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Weeks 1–5 — AI Usage Journal</title>
  <style>
    body {
      font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem;
      background: #fafafa;
      line-height: 1.6;
    }
    .wrap { max-width: 860px; margin: 0 auto; }
    .entry {
      background: #fff;
      padding: 2rem 2.5rem;
      border-radius: 16px;
      border: 1px solid #e5e7eb;
      box-shadow: 0 4px 14px rgba(0,0,0,.06);
    }
    h1 { margin-bottom: .25rem; }
    .muted { color: #6b7280; }
    h2 { margin-top: 1.75rem; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    ul { margin: .5rem 0 0 1.25rem; }
    code { background: #f3f4f6; padding: .1rem .35rem; border-radius: 6px; }
    .links a { display: inline-block; margin-right: .6rem; }
    .note { font-size: .92rem; color: #6b7280; margin-top: 1.25rem; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="entry">
      <h1>Weeks 1–5 — AI Usage Journal</h1>
      <p class="muted">INST326 · Messiah Khalfani · msnk@umd.edu</p>

      <h2>Week 1</h2>
      <p>
        This week I used AI while working on the exercises to explain certain Python concepts
        and to provide coding examples for Python concepts including <code>f-strings</code>,
        type conversion, and string slicing. I did not use AI to complete any of the exercises
        directly. I did not use AI for debugging.
      </p>
      <p class="links">
        Chat links:
        <a href="#" target="_blank" rel="noopener">Add link 1</a>
        <a href="#" target="_blank" rel="noopener">Add link 2</a>
      </p>
      <p>
        My project team also discussed ways to use AI to help organize our project ideas and
        translate them into project requirements. I compared outputs across tools and noticed
        responses vary even with similar prompts, which showed why prompt testing matters.
        Overall, I used AI as a support tool for explanations and brainstorming while keeping
        the actual coding and assignments my own.
      </p>

      <h2>Week 2</h2>
      <p>
        I used AI while working on the Week 2 Python exercises. I mainly asked for explanations
        of concepts like list indexing, loops, and simple debugging patterns, but I still wrote
        my own code in the <code>.ipynb</code> file.
      </p>
      <ul>
        <li><a href="#" target="_blank" rel="noopener">Loops and list slicing</a></li>
        <li><a href="#" target="_blank" rel="noopener">Formatting outputs</a></li>
      </ul>
      <p>
        I also checked how to document AI use so my journal format matches expectations. Our team
        briefly brainstormed project requirements with AI but did not copy answers directly.
      </p>

      <h2>Week 3</h2>
      <p>
        I used AI to help with the functions/code reuse discussion. I asked for examples of turning
        repeated code into a helper function and how to explain the benefits in a short post.
      </p>
      <ul>
        <li><a href="#" target="_blank" rel="noopener">Discussion drafting</a></li>
        <li><a href="#" target="_blank" rel="noopener">Syntax check for conditionals/loops</a></li>
        <li><a href="#" target="_blank" rel="noopener">Journal setup sanity check</a></li>
      </ul>
      <p>Overall, I used AI for quick explanations and confirming my thinking. I still wrote the code and answers myself.</p>

      <h2>Week 4</h2>
      <p><strong>What I did with AI</strong></p>
      <ul>
        <li>Project design and architecture: planned high-level structure; AI suggested directory layout and module boundaries.</li>
        <li>Writing and clarity improvements: rephrased report sections for clarity and flow.</li>
        <li>Debugging and error tracing: found an off-by-one indexing issue in a nested loop.</li>
        <li>Prompt workflow: outline → expand → refine → polish for more structured outputs.</li>
        <li>Model comparisons: differences between tools for code vs. explanation tasks.</li>
      </ul>
      <p><strong>Reflections</strong></p>
      <ul>
        <li>More detailed prompts produced better-structured outputs.</li>
        <li>AI did not replace thinking; I still validated logic and assumptions.</li>
        <li>Revising AI text sometimes took longer than drafting from scratch.</li>
        <li>I documented where AI helped and kept the final voice my own.</li>
      </ul>

      <h2>Week 5</h2>
      <p><strong>What I used AI for</strong></p>
      <ul>
        <li>Encapsulation review for a Binary Search Tree class; changed public fields to private and added getters.</li>
        <li>RA event planning for “Fall Into Success”; updated time to 7 PM and removed a snacks line.</li>
        <li>Community mapping bullets; ensured unique bullets and correct pronouns.</li>
        <li>GitHub Pages maintenance; added Week 4 and fixed navigation links.</li>
        <li>Small admin edits; step-by-step changes to dates and amounts on flyers/receipts.</li>
        <li>Rewording discussion posts to sound more like me (more casual, natural tone).</li>
      </ul>
      <p><strong>Representative chat links</strong></p>
      <ul>
        <li><a href="#" target="_blank" rel="noopener">Encapsulation review</a></li>
        <li><a href="#" target="_blank" rel="noopener">Event text edits</a></li>
        <li><a href="#" target="_blank" rel="noopener">Community mapping</a></li>
        <li><a href="#" target="_blank" rel="noopener">GitHub HTML fix</a></li>
        <li><a href="#" target="_blank" rel="noopener">Tone rewrite</a></li>
      </ul>
      <p><strong>Reflection</strong><br>
        AI was useful for structure and polishing, but I still reviewed all logic and presentation myself.
        Using it in moderation kept my work authentic while saving time on repetitive edits.
      </p>

      <p class="note">Replace the “Add link” placeholders with your real chat links.</p>
    </div>
  </div>
</body>
</html>
